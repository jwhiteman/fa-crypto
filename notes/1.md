So you've taken time away, and want to start this again:

# SET 1

1. this double reduce pattern (while not super performant) can make for
elegant solutions in some cases, especially when combined with min/max:

```
col.reduce([acc1, acc2]) do |(acc1, acc2), e|
  [new_acc1, new_acc2]
end[idx-of-the-thing-you-want]
```

### example - mapfold
```
input.reduce([0, []]) do |(prev, acc), e|
  [n = e + prev, acc << n]
end[1]
```

note that you can do "end.min[1]" or "end.max[1]", etc and it will sort
by acc1, and you can take acc2. useful.

2. pack("m"), unpack1("m"), and the variants with "H" and "C" are all useful.

3. cracking repeat key xor:

motivation: you've got some ciphertext you believe that has been xor'd
with a relativly short repeating key. you'll aim to crack the key.

- hamming/edit distance: for two bytes, xor them FIRST, then sum
  ((xor >> n) & 1) over 8 iterations
- normalized hamming distance: hamming/edit distance divided by num bytes
- reminder: you can simply use 1 - no need for 0b1 or 0b00000001 and the like
- first you need to figure out the keysize; in reality, you'd probably end
  up with several candidates to try, and not a single smoking gun....
- if you break up the ciphertext into KEYSIZE blocks, they will have a smaller
  normalized edit distance than different keysize blocks.
- for me to get this to work, I've needed to take the average of the
  normalized hamming distance for every single contiguous block. is there
  a better way? yeah - maybe. see 6.
- once the keysize is known (or you've got some nice guesses), then transpose
  the K sized blocks so that the nth bit of each once is aligned (e.g a block
  of only first bits, a block of only second bits, etc). Then for each block
  crack single byte xor.
- single byte xor: iterate 0..255, and score them (etaoin shrdlu) - highest
  inws.
  

NOTES ON WHY CALCULATING HAMMING DISTANCE WORKS:
- https://is.gd/LX011X
- concept 1: the edit distance of plaintext is lower than the edit distance
  of random/scambled bytes:

a:   "01100001"
f    "01100110"
240: "11110000"

the edit distance between a & f is 3
the edit distance between a & 240 is 3
the edit distance between f & 240 is 4

this maybe isn't a great example, but we can think that most english text
is made up of a-z, and even then with a bias towards etaoin shrdlu. random
bytes will take up all the space, and on average, be further apart.
- concept 2: if we have two different ciphertext blocks of the correct keysize:
  then we have (a[n] ^ k[n]) and (b[n] ^ k[n]), for all 0 <= n < keysize.
  this is equal to a[n] ^ b[n], e.g the hamming distance between two plaintext
  blocks. If we don't have the right keysize, it will be the xor of horeshit
  (not the plaintext) and thus the entire bytespace.

4. on detecting ECB
- look for duplicate blocks of 16 bytes (would be 32 chars, for example, if
  it was hex encoded). should be trivial if you've got an oracle.
